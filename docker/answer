#!/usr/bin/env python3
# -*- coding:utf8 -*-
import sys

from processor.stanza_process import StanzaProcessor
from ans.how_what_why_answer import HowWhatWhyAnswer
from ans.who_when_where_answer import WhoWhenWhereAnswer
from ans.yes_no_answer import YesNoAnswer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class Answer:
    def __init__(self, article, questions):
        self.questions = questions
        self.article = article
        self.answers = []
        self.tfidf = {}

    def calculate_tfidf(self, article):
        # Initialize a vectorizer that removes English stop words
        vectorizer = TfidfVectorizer()

        # Create a corpus of query and documents and convert to TFIDF vectors
        questions_and_article = self.questions + article
        tfidf = vectorizer.fit_transform(questions_and_article)
        for i, sentence in enumerate(questions_and_article):
            self.tfidf[sentence] = tfidf[i]

    def find_sentence(self, question, article):
        """
            TODO: find the sentence in article
                ideas: using tf-idf, cosine similarity... 
        """
        # Holds our cosine similarity scores
        scores = []

        # The first vector is our query text, so compute the similarity of our query against all document vectors
        for i in range(len(self.article)):
            scores.append(cosine_similarity(self.tfidf[question], self.tfidf[self.article[i]])[0][0])

        # Sort list of scores and return the top k highest scoring documents
        sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)
        
        return self.article[sorted_list[0][0]]
    

    def classify_and_answer(self, question, sentence):
        """
            TODO: classify question into different types using leading word
            then process the sentence using libraries
                Binary: Is/are/was/were/did/do/does/has/had/have...
                    - 我们可以先用随机来回答binary问题，后面再进一步改动
            --------------
                Who/Whom    NER Person
                When        NER Date
                Where       NER Location
            ---------------
                What/Which
                    - is/are..  
                    - else      
                How
                    - is/are..  
                    - many/much
                    - else      
                Why
        """
        stanza = StanzaProcessor(raw=sentence).process()
        processed_sentence = stanza.sentences[0]
        stanza = StanzaProcessor(raw=question).process()
        processed_question = stanza.sentences[0]

        first_word = processed_question.words[0].text.lower()
        if first_word == 'what' or first_word == 'which':
            return HowWhatWhyAnswer(processed_sentence, processed_question).what()
        elif first_word == 'how':
            return HowWhatWhyAnswer(processed_sentence, processed_question).how()
        elif first_word == 'why':
            return HowWhatWhyAnswer(processed_sentence, processed_question).why()
        elif first_word == 'who' or first_word == 'whom':
            return WhoWhenWhereAnswer(processed_sentence, processed_question).who()
        elif first_word == 'when':
            return WhoWhenWhereAnswer(processed_sentence, processed_question).when()
        elif first_word == 'where':
            return WhoWhenWhereAnswer(processed_sentence, processed_question).where()
        else:
            generate = YesNoAnswer(processed_sentence, processed_question)
            return generate.yesno()
    
    def generate(self):
        self.calculate_tfidf(self.article)

        for question in self.questions:
            
            sentence = self.find_sentence(question, self.article)
            answer = self.classify_and_answer(question, sentence)
            self.answers.append(answer)

            # for testing
            print(answer)

    

if __name__ == "__main__":
    # article_file = "../data/set1/a1.txt"
    article_file = sys.argv[1]
    processor = StanzaProcessor(file=article_file)
    processor.sentence_segmentation()
    # processor.process()
    article = processor.sentences
    # print(article)

    # question_file = "../test_questions.txt"
    question_file = sys.argv[2]
    questions = []
    with open(question_file, 'r') as f:
        count = 0
        for line in f:
            count += 1
            line = line.replace("?", "") # delete question mark
            questions.append(line.strip())
    answer = Answer(article, questions)
    answer.generate()